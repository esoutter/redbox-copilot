{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import UUID\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from dataclasses import asdict\n",
    "from pathlib import Path\n",
    "import jsonlines\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from redbox.models import Settings\n",
    "from redbox.models.settings import ElasticLocalSettings\n",
    "from redbox.storage.elasticsearch import hit_to_chunk\n",
    "from redbox.models import Settings\n",
    "\n",
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain.globals import set_verbose\n",
    "\n",
    "from collections.abc import Callable\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "from redbox.models import Settings\n",
    "from redbox.models.file import UUID\n",
    "from redbox.storage.elasticsearch import hit_to_chunk\n",
    "\n",
    "\n",
    "set_verbose(False)\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "ENV = Settings(minio_host=\"localhost\", elastic=ElasticLocalSettings(host=\"localhost\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "☝️⚠️ _expand cell to run imports_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation notebook for trying to improve Redbox RAG chat  <a class=\"anchor\" id=\"title\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Redbox RAG chat on one stable, numbered version of these data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
    "\n",
    "1. Introduction\n",
    "\n",
    "* [Overview](#overview)\n",
    "* [Metrics](#metrics)\n",
    "    - [Contextual Precisio]()\n",
    "    - [Contextual Recall]()\n",
    "    - [Contextual Relevancy]()\n",
    "    - [Fathfulness]()\n",
    "    - [Answer Relevancy]()\n",
    "    - [Hallucination]()\n",
    "    \n",
    "2. Setup\n",
    "\n",
    "* [Set version of the evaluation dataset](#setversion)\n",
    "* [Run Redbox locally](#run-redbox)\n",
    "* [Load embeddings into the index](#load-embeddings)\n",
    "\n",
    "3. Experiment\n",
    "\n",
    "* [a. Get files that correspond to the version of evaluation dataset](#files)\n",
    "* [b. Load Evaluation Dataset into test cases](#load-test-cases)\n",
    "* [c. Generate `actual_output` using RAG and evaluation dataset](#evaluate)\n",
    "    - [Retrieval Evaluation Metrics]()\n",
    "    - [Generation Evaluation Metrics]()\n",
    "* [d. Analyse evaluation results](#analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview <a class=\"anchor\" id=\"overview\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows you to experiment with the retrieval and generation parts of Redbox RAG chat **WITHIN** the notebook, and get back evaluation metrics quickly. This allows you to test if the changes you make can improve the evaluation metrics, compared against the current/deployed RAG chat endpoint.\n",
    "\n",
    "Redbox RAG chat is made up of many components that work together to give the final RAG pipeline. Each component can be optimised, to hopefully improve the over all performance of the RAG pipeline for Redbox tasks. In order to track if changes made are improving or degrading Redbox performance, we need to establish an evaluation framework. The overall RAG pipeline can be broken down into two main parts:\n",
    "\n",
    "1. Retrieval - searching and returning the most relevant documents to answer a user question\n",
    "2. Generation - the ouput of the LLM after considering the retrieved documents, any prompts provided and the user question\n",
    "\n",
    "This notebook tests both the retrieval and generation sides of the RAG pipeline using specific metrics for each, using the `DeepEval` framework.\n",
    "\n",
    "\n",
    "For consistency across the team, it is important to evaluate Redbox RAG chat on one stable, numbered version of these data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics <a class=\"anchor\" id=\"metrics\"></a>\n",
    "\n",
    "Retrieval metrics\n",
    "- Contextual Precision\n",
    "- Contextual Recall\n",
    "- Contextual Relevancy\n",
    "\n",
    "Generation metrics\n",
    "- Faithfulness\n",
    "- Answer Relevancy\n",
    "- Hallucination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contextual Precision\n",
    "\n",
    "The contextual precision metric measures your RAG pipeline's retriever by evaluating whether nodes in your `retrieval_context` that are relevant to the given `input` are ranked higher than irrelevant ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contextual Recall\n",
    "\n",
    "The contextual recall metric measures the quality of your RAG pipeline's retriever by evaluating the extent of which the `retrieval_context` aligns with the `expected_output`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contextual Relevancy\n",
    "\n",
    "The contextual relevancy metric measures the quality of your RAG pipeline's retriever by evaluating the overall relevance of the information presented in your `retrieval_context` for a given `input`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faithfulness\n",
    "\n",
    "The faithfulness metric measures the quality of your RAG pipeline's generator by evaluating whether the `actual_output` factually aligns with the contents of your `retrieval_context`. `deepeval`'s faithfulness metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Arguments\n",
    "To use the `FaithfulnessMetric`, you need to provide the following arguments when creating an LLMTestCase:\n",
    "\n",
    "- `input`\n",
    "- `actual_output`\n",
    "- `retrieval_context`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Relevancy\n",
    "The answer relevancy metric measures the quality of your RAG pipeline's generator by evaluating how relevant the actual_output of your LLM application is compared to the provided `input`. `deepeval`'s answer relevancy metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Arguments\n",
    "To use the AnswerRelevancyMetric, you'll have to provide the following arguments when creating an LLMTestCase:\n",
    "\n",
    "- `input`\n",
    "- `actual_output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hallucination\n",
    "The hallucination metric determines whether your LLM generates factually correct information by comparing the `actual_output` to the provided `context`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Arguments\n",
    "To use the HallucinationMetric, you'll have to provide the following arguments when creating an LLMTestCase:\n",
    "\n",
    "- `input`\n",
    "- `actual_output`\n",
    "- `retrieval_context`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Redbox RAG chat on one stable, numbered version of these data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the version of the evaluation dataset you are using to evalute Redbox in the cell below**   <a class=\"anchor\" id=\"setversion\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_VERSION = \"0.2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding and retrieval is locked to a particular embedding model, which should be tied to a single index in the vector stoer. Here we default to the `EMBEDDING_MODEL` environment variable, which will match production if set via `.env.example`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = ENV.embedding_model\n",
    "INDEX = f\"{DATA_VERSION}-{MODEL}\".lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to set up the required folder structure (it will not overwrite folders and files if they already exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path.cwd().parents[1]\n",
    "EVALUATION_DIR = ROOT / \"notebooks/evaluation\"\n",
    "\n",
    "V_ROOT = EVALUATION_DIR / f\"data/{DATA_VERSION}\"\n",
    "V_RAW = V_ROOT / \"raw\"\n",
    "V_SYNTHETIC = V_ROOT / \"synthetic\"\n",
    "V_CHUNKS = V_ROOT / \"chunks\"\n",
    "V_RESULTS = V_ROOT / \"results\"\n",
    "V_EMBEDDINGS = V_ROOT / \"embeddings\"\n",
    "\n",
    "V_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "V_RAW.mkdir(parents=True, exist_ok=True)\n",
    "V_SYNTHETIC.mkdir(parents=True, exist_ok=True)\n",
    "V_CHUNKS.mkdir(parents=True, exist_ok=True)\n",
    "V_RESULTS.mkdir(parents=True, exist_ok=True)\n",
    "V_EMBEDDINGS.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save on API costs, we only need to generate a particular version of the evaluation dataset once. If you are using a previously generaterated evalutation dataset, **please download it from shared team location (Google Drive).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's helpful for all calls to share a dummy user. Set that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_UUID = UUID(\"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the clients to connect to the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_CLIENT = ENV.s3_client()\n",
    "ES_CLIENT = ENV.elasticsearch_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Redbox locally <a id=\"run-redbox\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start docker runtime, likely with Docker Desktop. However, if you are using colima run the following terminal command\n",
    "\n",
    "```bash\n",
    "colima start --memory 8\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First-time setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First time users need to do the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "poetry install\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure your `.env` file has an OpenAI API key in, and has the following settings.\n",
    "\n",
    "Note `EMBEDDING_MODEL` must also be set, and should match both production, and the embeddings shared with the dataset. If not, go back to dataset creation, and embed the documents using a model that matches production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# === Object Storage ===\n",
    "\n",
    "MINIO_HOST=minio\n",
    "MINIO_PORT=9000\n",
    "MINIO_ACCESS_KEY=minioadmin\n",
    "MINIO_SECRET_KEY=minioadmin\n",
    "AWS_ACCESS_KEY=minioadmin\n",
    "AWS_SECRET_KEY=minioadmin\n",
    "\n",
    "AWS_REGION=eu-west-2\n",
    "\n",
    "# minio or s3\n",
    "OBJECT_STORE=minio\n",
    "BUCKET_NAME=redbox-storage-dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Redbox locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Every time you start Redbox for evaluation (no Django frontend required), please run the following command**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "make eval_backend\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command will bring up everything you need for the backend (`core-api`, `worker`, `mino`, `elasticsearch` and `redis`), then create the MinIO bucket needed to store raw files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embeddings into the index and get file UUIDs <a id=\"load-embeddings\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks_from_jsonl_to_index(file_path: Path, es_client: Elasticsearch, index: str) -> set:\n",
    "\n",
    "    file_uuids = set()\n",
    "\n",
    "    with jsonlines.open(file_path, mode=\"r\") as reader:\n",
    "        for chunk_raw in reader:\n",
    "            chunk = json.loads(chunk_raw)\n",
    "            es_client.index(\n",
    "                index=index,\n",
    "                id=chunk[\"uuid\"],\n",
    "                body=chunk,\n",
    "            )\n",
    "\n",
    "            file_uuids.add(chunk[\"parent_file_uuid\"])\n",
    "\n",
    "    return file_uuids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_UUIDS = load_chunks_from_jsonl_to_index(file_path=V_EMBEDDINGS / f\"{MODEL}.jsonl\", es_client=ES_CLIENT, index=INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment\n",
    "\n",
    "Use code below to experiment, in order to improve evaluation metrics or address a performance issue\n",
    "\n",
    "For setting an initial baseline with the existing Redbox Core API endpoint, please start [HERE](#baseline)\n",
    "\n",
    "If you've already uploaded documents you can skip to [the experimentation phase](#evaluate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Generate `actual_output` and `retrieval_context` using in-notebook `rag_chat()` function <a id=\"evaluate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{V_SYNTHETIC}/ragas_synthetic_data.csv\")\n",
    "inputs = df[\"input\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using a `rag_chat()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conceptualise RAG as having four mechanisms we might tune:\n",
    "\n",
    "* Chunking\n",
    "* Embedding\n",
    "* Retriever\n",
    "* Prompts\n",
    "\n",
    "The below `rag_chat()` function replicates the internal logic of the RAG endpoint. By editing and using it here, you can quickly iterate and test the retriever and prompt mechanisms using your stable, versioned data, giving sharable, reproducible results.\n",
    "\n",
    "As long as `rag_chat()` takes a question (and history) and produces an answer, it's a testable process that could be used in Redbox. Everything within the function is yours to play with -- prompts, retriever, everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the experiment name that you want to associate with your experiment in the cell below** - this will save the output scores with this experiment in the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"original_prompt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core_api.src import dependencies\n",
    "from redbox.models import ChatRoute, Chunk, Settings\n",
    "from redbox.models.chain import ChainInput\n",
    "\n",
    "from typing import Annotated, Any\n",
    "from fastapi import Depends\n",
    "from tiktoken import Encoding\n",
    "from langchain_core.runnables import Runnable, RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from operator import itemgetter\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "from core_api.src.format import format_documents\n",
    "from core_api.src.runnables import make_chat_prompt_from_messages_runnable\n",
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from core_api.src.dependencies import get_parameterised_retriever, get_tokeniser\n",
    "from core_api.src.retriever import ParameterisedElasticsearchRetriever\n",
    "\n",
    "LLM = ChatLiteLLM(\n",
    "    model=\"gpt-4o\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "RETRIEVAL_SYSTEM_PROMPT = (\n",
    "    \"Given the following conversation and extracted parts of a long document and a question, create a final answer. \\n\"\n",
    "    \"If you don't know the answer, just say that you don't know. Don't try to make up an answer. \"\n",
    "    \"If a user asks for a particular format to be returned, such as bullet points, then please use that format. \"\n",
    "    \"If a user asks for bullet points you MUST give bullet points. \"\n",
    "    \"If the user asks for a specific number or range of bullet points you MUST give that number of bullet points. \\n\"\n",
    "    \"Use **bold** to highlight the most question relevant parts in your response. \"\n",
    "    \"If dealing dealing with lots of data return it in markdown table format. \"\n",
    ")\n",
    "\n",
    "RETRIEVAL_QUESTION_PROMPT = \"{question} \\n=========\\n{formatted_documents}\\n=========\\nFINAL ANSWER: \"\n",
    "\n",
    "def get_parameterised_retriever(\n",
    "    env: Annotated[Settings, Depends(ENV)], es: Annotated[Elasticsearch, Depends(dependencies.get_elasticsearch_client)]\n",
    ") -> BaseRetriever:\n",
    "    \"\"\"Creates an Elasticsearch retriever runnable.\n",
    "\n",
    "    Runnable takes input of a dict keyed to question, file_uuids and user_uuid.\n",
    "\n",
    "    Runnable returns a list of Chunks.\n",
    "    \"\"\"\n",
    "    default_params = {\n",
    "        \"size\": env.ai.rag_k,\n",
    "        \"num_candidates\": env.ai.rag_num_candidates,\n",
    "        \"match_boost\": 1,\n",
    "        \"knn_boost\": 1,\n",
    "        \"similarity_threshold\": 0,\n",
    "    }\n",
    "    return ParameterisedElasticsearchRetriever(\n",
    "        es_client=es,\n",
    "        index_name=INDEX,\n",
    "        params=default_params,\n",
    "        embedding_model=dependencies.get_embedding_model(env),\n",
    "    ).configurable_fields(\n",
    "        params=ConfigurableField(\n",
    "            id=\"params\", name=\"Retriever parameters\", description=\"A dictionary of parameters to use for the retriever.\"\n",
    "        )\n",
    "    )\n",
    "      \n",
    "              \n",
    "def build_retrieval_chain(\n",
    "    llm: Annotated[ChatLiteLLM, Depends(dependencies.get_llm)],\n",
    "    retriever: Annotated[VectorStoreRetriever, Depends(dependencies.get_parameterised_retriever)],\n",
    "    tokeniser: Annotated[Encoding, Depends(dependencies.get_tokeniser)],\n",
    "    env: Annotated[Settings, Depends(dependencies.get_env)],\n",
    ") -> Runnable:\n",
    "    return (\n",
    "        RunnablePassthrough.assign(documents=retriever)\n",
    "        | RunnablePassthrough.assign(\n",
    "            formatted_documents=(RunnablePassthrough() | itemgetter(\"documents\") | format_documents)\n",
    "        )\n",
    "        | {\n",
    "            \"response\": make_chat_prompt_from_messages_runnable(\n",
    "                system_prompt=env.ai.retrieval_system_prompt,\n",
    "                question_prompt=env.ai.retrieval_question_prompt,\n",
    "                input_token_budget=env.ai.context_window_size - env.llm_max_tokens,\n",
    "                tokeniser=tokeniser,\n",
    "            )\n",
    "            | llm\n",
    "            | StrOutputParser(),\n",
    "            \"source_documents\": itemgetter(\"documents\"),\n",
    "            \"route_name\": RunnableLambda(lambda _: ChatRoute.search.value),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def get_rag_results(question: str\n",
    "                    ) -> dict:\n",
    "    \n",
    "    '''Get Redbox response for a given question.'''\n",
    "    \n",
    "    retriever = get_parameterised_retriever(es=ES_CLIENT, env=ENV)\n",
    "\n",
    "    chain = build_retrieval_chain(llm=LLM, retriever=retriever, tokeniser=get_tokeniser(), env=ENV)\n",
    "    \n",
    "    response = chain.invoke(\n",
    "        input=ChainInput(\n",
    "            question=question,\n",
    "            chat_history = [{\"text\": \"\", \"role\": \"user\"}],\n",
    "            file_uuids=list(FILE_UUIDS),\n",
    "            user_uuid=USER_UUID,\n",
    "        ).model_dump()\n",
    "    )\n",
    "\n",
    "    filtered_chunks = []\n",
    "\n",
    "    for chunk in response['source_documents']:\n",
    "\n",
    "        chunk = dict(chunk)\n",
    "        filtered_chunk = {'page_content': chunk['page_content'], 'page_number': chunk['metadata']['page_number'], 'parent_file_uuid': chunk['metadata']['parent_file_uuid']}\n",
    "        filtered_chunks.append(filtered_chunk)\n",
    "\n",
    "    return {\"output_text\": response[\"response\"], \"source_documents\": filtered_chunks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "df_function = df.copy()\n",
    "\n",
    "retrieval_context = []\n",
    "actual_output = []\n",
    "\n",
    "for question in inputs:\n",
    "    data = get_rag_results(question=question)\n",
    "\n",
    "    retrieval_context.append(data[\"source_documents\"])\n",
    "    actual_output.append(data[\"output_text\"])\n",
    "\n",
    "df_function[\"actual_output\"] = actual_output\n",
    "df_function[\"retrieval_context\"] = retrieval_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm `actual_output` and `retrieved_context` added to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_function.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rows containing NaN to prevent Pydantic validation errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_function_clean = df_function.dropna()\n",
    "df_function_clean.to_csv(f\"{V_SYNTHETIC}/{EXPERIMENT_NAME}_complete_ragas_synthetic_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Load evaluation dataset into test cases <a class=\"anchor\" id=\"load-test-cases\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the CSV file that you want to use for evaluation into `/notebooks/evaluation/data/synthetic_data/` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import test cases from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "dataset = EvaluationDataset()\n",
    "dataset.add_test_cases_from_csv_file(\n",
    "    file_path=f\"{V_SYNTHETIC}/{EXPERIMENT_NAME}_complete_ragas_synthetic_data.csv\",  # function\n",
    "    input_col_name=\"input\",\n",
    "    actual_output_col_name=\"actual_output\",\n",
    "    expected_output_col_name=\"expected_output\",\n",
    "    context_col_name=\"context\",\n",
    "    context_col_delimiter=\";\",\n",
    "    retrieval_context_col_name=\"retrieval_context\",\n",
    "    retrieval_context_col_delimiter=\";\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Evaluate RAG pipeline <a id=\"evaluate\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepEval imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    HallucinationMetric,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate retrieval and generation evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate retrieval metrics\n",
    "contextual_precision = ContextualPrecisionMetric(\n",
    "    threshold=0.5,  # default is 0.5\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    ")\n",
    "\n",
    "contextual_recall = ContextualRecallMetric(\n",
    "    threshold=0.5,  # default is 0.5\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    ")\n",
    "\n",
    "contextual_relevancy = ContextualRelevancyMetric(\n",
    "    threshold=0.5,  # default is 0.5\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate generation metrics\n",
    "answer_relevancy = AnswerRelevancyMetric(\n",
    "    threshold=0.5,  # default is 0.5\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    ")\n",
    "\n",
    "faithfulness = FaithfulnessMetric(\n",
    "    threshold=0.5,  # default is 0.5\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    ")\n",
    "\n",
    "hallucination = HallucinationMetric(\n",
    "    threshold=0.5,  # default is 0.5\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.test_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = evaluate(\n",
    "    test_cases=dataset,\n",
    "    metrics=[\n",
    "        contextual_precision,\n",
    "        contextual_recall,\n",
    "        contextual_relevancy,\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        hallucination,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{V_RESULTS}/{EXPERIMENT_NAME}_generation_eval_results\", \"wb\") as f:\n",
    "    pickle.dump(eval_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{V_RESULTS}/{EXPERIMENT_NAME}_generation_eval_results\", \"rb\") as f:\n",
    "    eval_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Analyse evaluation results <a id=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_type = {\n",
    "    \"metric_name\": [\n",
    "        \"Contextual Precision\",\n",
    "        \"Contextual Recall\",\n",
    "        \"Contextual Relevancy\",\n",
    "        \"Answer Relevancy\",\n",
    "        \"Faithfulness\",\n",
    "        \"Hallucination\",\n",
    "    ],\n",
    "    \"metric_type\": [\"retrieval\", \"retrieval\", \"retrieval\", \"generation\", \"generation\", \"generation\"],\n",
    "}\n",
    "\n",
    "evaluation = (\n",
    "    pd.DataFrame.from_records(asdict(result) for result in eval_results)\n",
    "    .explode(\"metrics_metadata\")\n",
    "    .reset_index(drop=True)\n",
    "    .assign(\n",
    "        metric_name=lambda df: df.metrics_metadata.apply(getattr, args=[\"metric\"]),\n",
    "        score=lambda df: df.metrics_metadata.apply(getattr, args=[\"score\"]),\n",
    "        reason=lambda df: df.metrics_metadata.apply(getattr, args=[\"reason\"]),\n",
    "    )\n",
    "    .merge(pd.DataFrame(metric_type), on=\"metric_name\")\n",
    "    .drop(columns=[\"success\", \"metrics_metadata\"])\n",
    ")\n",
    "\n",
    "evaluation.to_csv(f\"{V_RESULTS}/{EXPERIMENT_NAME}_generation_eval_results.csv\", index=False)\n",
    "evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(evaluation.groupby([\"metric_name\", \"metric_type\"]).mean(\"score\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare experiments against baseline or visualise baseline alone\n",
    "Note: there are some complexities that could require additional analysis, such as the uncertainty associated with each individual LLM judge score and the wide range of scores (0 to 1 for some metrics; see boxplot below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "experiments = []\n",
    "\n",
    "baseline = pd.read_csv(f\"{V_RESULTS}/baseline.csv\")\n",
    "baseline[\"experiment_name\"] = \"baseline\"\n",
    "experiments.append(baseline)\n",
    "\n",
    "# Comment out if you only want to view baseline statistics\n",
    "# or populate with experiment names\n",
    "experiment_names = ['original_prompt']\n",
    "for experiment_name in experiment_names:\n",
    "    experiment = pd.read_csv(f\"{V_RESULTS}/{experiment_name}_generation_eval_results.csv\")\n",
    "    experiment['experiment_name'] = experiment_name\n",
    "    experiments.append(experiment)\n",
    "\n",
    "experiments_df = pd.concat(experiments)\n",
    "\n",
    "def empirical_ci(df: pd.DataFrame\n",
    "                 ) -> pd.DataFrame:\n",
    "\n",
    "    '''Calculate confidence intervals for aggregated metrics.'''\n",
    "\n",
    "    df_grouped = (df\n",
    "                  .groupby([\"experiment_name\", \"metric_name\"])['score']\n",
    "                  .agg([\"mean\", 'sem', 'min', 'max', 'count'])\n",
    "                  .reset_index()\n",
    "                  )\n",
    "        \n",
    "    ci = stats.t.interval(confidence=0.95, \n",
    "                          df=df_grouped['count']-1,\n",
    "                          loc=df_grouped['mean'],\n",
    "                          scale=df_grouped['sem'])\n",
    "\n",
    "    df_grouped['ci_low'] = ci[0]\n",
    "    df_grouped['ci_high'] = ci[1] \n",
    "\n",
    "    return df_grouped\n",
    "\n",
    "# Note that the confidence intervals in sns.barplot are calculated by bootstrapping.\n",
    "# See empirical_ci() above for empirical confidence interval calculation.\n",
    "sns.barplot(experiments_df, x=\"score\", y=\"metric_name\", hue=\"experiment_name\", errorbar=(\"ci\", 95))\n",
    "\n",
    "experiment_metrics = empirical_ci(experiments_df)\n",
    "experiment_metrics.to_csv(f\"{V_RESULTS}/{experiment_name}_eval_results_full.csv\")\n",
    "experiment_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplot of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(experiments_df, x=\"score\", y=\"metric_name\", hue=\"experiment_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THIS IS THE END OF THE EXPERIMENT**\n",
    "\n",
    "Return to [HERE](#evaluate) to try a new experiment\n",
    "\n",
    "The section below is to establish a baseline with the current Core API endpoint only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-MiicHf1r-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
