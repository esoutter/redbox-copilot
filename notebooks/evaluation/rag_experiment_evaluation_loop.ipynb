{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation Experiment Loop\n",
    "\n",
    "This notebook reproduces the workflow in [rag_experiment_evaluation.ipynb](rag_experiment_evaluation.ipynb) through a series of functions. \n",
    "\n",
    "These functions allow us to loop through a CSV of experiments in order to systematically change prompts and measure the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import UUID\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from dataclasses import asdict\n",
    "from pathlib import Path\n",
    "import jsonlines\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from redbox.models import Settings\n",
    "from redbox.models.settings import ElasticLocalSettings\n",
    "from redbox.storage.elasticsearch import hit_to_chunk\n",
    "from redbox.models import Settings\n",
    "\n",
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain.globals import set_verbose\n",
    "\n",
    "from collections.abc import Callable\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "from redbox.models import Settings\n",
    "from redbox.models.file import UUID\n",
    "from redbox.storage.elasticsearch import hit_to_chunk\n",
    "\n",
    "\n",
    "set_verbose(False)\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "ENV = Settings(minio_host=\"localhost\", elastic=ElasticLocalSettings(host=\"localhost\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set evaluation data version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_VERSION = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set paths and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path.cwd().parents[1]\n",
    "EVALUATION_DIR = ROOT / \"notebooks/evaluation\"\n",
    "\n",
    "V_ROOT = EVALUATION_DIR / f\"data/{DATA_VERSION}\"\n",
    "V_RAW = V_ROOT / \"raw\"\n",
    "V_SYNTHETIC = V_ROOT / \"synthetic\"\n",
    "V_CHUNKS = V_ROOT / \"chunks\"\n",
    "V_RESULTS = V_ROOT / \"results\"\n",
    "V_EMBEDDINGS = V_ROOT / \"embeddings\"\n",
    "\n",
    "V_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "V_RAW.mkdir(parents=True, exist_ok=True)\n",
    "V_SYNTHETIC.mkdir(parents=True, exist_ok=True)\n",
    "V_CHUNKS.mkdir(parents=True, exist_ok=True)\n",
    "V_RESULTS.mkdir(parents=True, exist_ok=True)\n",
    "V_EMBEDDINGS.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = ENV.embedding_model\n",
    "INDEX = f\"{DATA_VERSION}-{MODEL}\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_UUID = UUID(\"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\")\n",
    "S3_CLIENT = ENV.s3_client()\n",
    "ES_CLIENT = ENV.elasticsearch_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load embeddings into the index and get file UUIDs <a id=\"load-embeddings\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks_from_jsonl_to_index(file_path: Path, es_client: Elasticsearch, index: str) -> set:\n",
    "\n",
    "    file_uuids = set()\n",
    "\n",
    "    with jsonlines.open(file_path, mode=\"r\") as reader:\n",
    "        for chunk_raw in reader:\n",
    "            chunk = json.loads(chunk_raw)\n",
    "            es_client.index(\n",
    "                index=index,\n",
    "                id=chunk[\"uuid\"],\n",
    "                body=chunk,\n",
    "            )\n",
    "\n",
    "            file_uuids.add(chunk[\"parent_file_uuid\"])\n",
    "\n",
    "    return file_uuids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_UUIDS = load_chunks_from_jsonl_to_index(file_path=V_EMBEDDINGS / f\"{MODEL}.jsonl\", es_client=ES_CLIENT, index=INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define function to get RAG outputs based on prompts in experiments \n",
    "Note: these can be made more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core_api.src.runnables import make_chat_prompt_from_messages_runnable\n",
    "from core_api.src import dependencies\n",
    "from core_api.src.format import format_chunks\n",
    "from redbox.models import ChatRoute, Chunk, Settings\n",
    "from core_api.src.dependencies import get_es_retriever\n",
    "from redbox.models.chain import ChainInput\n",
    "\n",
    "from typing import Annotated, Any\n",
    "from fastapi import Depends\n",
    "from langchain_core.runnables import Runnable, RunnableLambda, RunnablePassthrough, chain\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from operator import itemgetter\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_elasticsearch import ElasticsearchRetriever\n",
    "\n",
    "LLM = ChatLiteLLM(\n",
    "    model=\"gpt-4o\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "\n",
    "def get_es_retriever(\n",
    "    env: Annotated[Settings, Depends(dependencies.get_env)], es: Annotated[Elasticsearch, Depends(dependencies.get_elasticsearch_client)]\n",
    ") -> BaseRetriever:\n",
    "    \"\"\"Creates an Elasticsearch retriever runnable.\n",
    "\n",
    "    Runnable takes input of a dict keyed to question, file_uuids and user_uuid.\n",
    "\n",
    "    Runnable returns a list of Chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    def es_query(query: dependencies.ESQuery, params: dependencies.ESParams) -> dict[str, Any]:\n",
    "        vector = dependencies.get_embedding_model(env).embed_query(query[\"question\"])\n",
    "\n",
    "        query_filter = [\n",
    "            {\n",
    "                \"bool\": {\n",
    "                    \"should\": [\n",
    "                        {\"term\": {\"creator_user_uuid.keyword\": str(query[\"user_uuid\"])}},\n",
    "                        {\"term\": {\"metadata.creator_user_uuid.keyword\": str(query[\"user_uuid\"])}},\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        if len(query[\"file_uuids\"]) != 0:\n",
    "            query_filter.append(\n",
    "                {\n",
    "                    \"bool\": {\n",
    "                        \"should\": [\n",
    "                            {\"terms\": {\"parent_file_uuid.keyword\": [str(uuid) for uuid in query[\"file_uuids\"]]}},\n",
    "                            {\n",
    "                                \"terms\": {\n",
    "                                    \"metadata.parent_file_uuid.keyword\": [str(uuid) for uuid in query[\"file_uuids\"]]\n",
    "                                }\n",
    "                            },\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"size\": params[\"size\"],\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"should\": [\n",
    "                        {\n",
    "                            \"match\": {\n",
    "                                \"text\": {\n",
    "                                    \"query\": query[\"question\"],\n",
    "                                    \"boost\": params[\"match_boost\"],\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"knn\": {\n",
    "                                \"field\": \"embedding\",\n",
    "                                \"query_vector\": vector,\n",
    "                                \"num_candidates\": params[\"num_candidates\"],\n",
    "                                \"filter\": query_filter,\n",
    "                                \"boost\": params[\"knn_boost\"],\n",
    "                                \"similarity\": params[\"similarity_threshold\"],\n",
    "                            }\n",
    "                        },\n",
    "                    ],\n",
    "                    \"filter\": query_filter,\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "\n",
    "    class ParameterisedElasticsearchRetriever(ElasticsearchRetriever):\n",
    "        params: dependencies.ESParams\n",
    "        body_func: Callable[[str], dict]\n",
    "\n",
    "        def __init__(self, **kwargs: Any) -> None:\n",
    "            super().__init__(**kwargs)\n",
    "            self.body_func = dependencies.partial(self.body_func, params=self.params)\n",
    "\n",
    "    default_params = {\n",
    "        \"size\": env.ai.rag_k,\n",
    "        \"num_candidates\": env.ai.rag_num_candidates,\n",
    "        \"match_boost\": 1,\n",
    "        \"knn_boost\": 1,\n",
    "        \"similarity_threshold\": 0,\n",
    "    }\n",
    "\n",
    "    return ParameterisedElasticsearchRetriever(\n",
    "        es_client=es,\n",
    "        index_name=INDEX,\n",
    "        body_func=es_query,\n",
    "        document_mapper=hit_to_chunk,\n",
    "        params=default_params,\n",
    "    ).configurable_fields(\n",
    "        params=ConfigurableField(\n",
    "            id=\"params\", name=\"Retriever parameters\", description=\"A dictionary of parameters to use for the retriever.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "def build_retrieval_chain(\n",
    "    llm: Annotated[ChatLiteLLM, Depends(dependencies.get_llm)],\n",
    "    retriever: Annotated[VectorStoreRetriever, Depends(dependencies.get_es_retriever)],\n",
    "    env: Annotated[Settings, Depends(dependencies.get_env)],\n",
    "    RETRIEVAL_SYSTEM_PROMPT: str,\n",
    "    RETRIEVAL_QUESTION_PROMPT: str,\n",
    ") -> Runnable:\n",
    "    return (\n",
    "        RunnablePassthrough.assign(documents=retriever)\n",
    "        | RunnablePassthrough.assign(\n",
    "            formatted_documents=(RunnablePassthrough() | itemgetter(\"documents\") | format_chunks)\n",
    "        )\n",
    "        | {\n",
    "            \"response\": make_chat_prompt_from_messages_runnable(\n",
    "                RETRIEVAL_SYSTEM_PROMPT, RETRIEVAL_QUESTION_PROMPT\n",
    "            )\n",
    "            | llm\n",
    "            | StrOutputParser(),\n",
    "            \"source_documents\": itemgetter(\"documents\"),\n",
    "            \"route_name\": RunnableLambda(lambda _: ChatRoute.search.value),\n",
    "        }\n",
    "    )\n",
    "def write_rag_results(V_SYNTHETIC: str, \n",
    "                      EXPERIMENT_NAME: str,  \n",
    "                      RETRIEVAL_SYSTEM_PROMPT:str, \n",
    "                      RETRIEVAL_QUESTION_PROMPT:str\n",
    "                      ) -> None:\n",
    "\n",
    "    def get_rag_results(question: str\n",
    "                        ) -> dict:\n",
    "\n",
    "        retriever = get_es_retriever(env=ENV,\n",
    "                                     es=ENV.elasticsearch_client())\n",
    "        \n",
    "        chain = build_retrieval_chain(llm=LLM,\n",
    "                                      retriever=retriever, \n",
    "                                      RETRIEVAL_SYSTEM_PROMPT=RETRIEVAL_SYSTEM_PROMPT,\n",
    "                                      RETRIEVAL_QUESTION_PROMPT=RETRIEVAL_QUESTION_PROMPT,\n",
    "                                      env=ENV)\n",
    "        \n",
    "        response = chain.invoke(\n",
    "            input=ChainInput(\n",
    "                question=question,\n",
    "                chat_history = [{\"text\": \"\", \"role\": \"user\"}],\n",
    "                file_uuids=list(FILE_UUIDS),\n",
    "                user_uuid=USER_UUID,\n",
    "            ).model_dump()\n",
    "        )\n",
    "\n",
    "        filtered_chunks = []\n",
    "\n",
    "        for chunk in response['source_documents']:\n",
    "\n",
    "            chunk = dict(chunk)\n",
    "            filtered_chunk = {'text': chunk['text'], 'uuid':chunk['uuid'], 'parent_file_uuid': chunk['parent_file_uuid']}\n",
    "            filtered_chunks.append(filtered_chunk)\n",
    "\n",
    "        return {\"output_text\": response[\"response\"], \"source_documents\": filtered_chunks}\n",
    "    \n",
    "    df = pd.read_csv(f\"{V_SYNTHETIC}/ragas_synthetic_data.csv\")\n",
    "    inputs = df[\"input\"].tolist()\n",
    "\n",
    "    df_function = df.copy()\n",
    "\n",
    "    actual_output = []\n",
    "    retrieval_context = []\n",
    "\n",
    "    for question in inputs:\n",
    "        \n",
    "        data = get_rag_results(question=question)\n",
    "        actual_output.append(data[\"output_text\"])\n",
    "        retrieval_context.append(data['source_documents'])\n",
    "\n",
    "    df_function[\"actual_output\"] = actual_output\n",
    "    df_function[\"retrieval_context\"] = retrieval_context\n",
    "\n",
    "    df_function_clean = df_function.dropna()\n",
    "    df_function_clean.to_csv(f\"{V_SYNTHETIC}/{EXPERIMENT_NAME}_complete_ragas_synthetic_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Define function to evaluation test dataset \n",
    "Note: these can be made more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import EvaluationDataset\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    HallucinationMetric,\n",
    ")\n",
    "\n",
    "def do_evaluation(V_SYNTHETIC: str, V_RESULTS: str, EXPERIMENT_NAME: str) -> None:\n",
    "\n",
    "    dataset = EvaluationDataset()\n",
    "    dataset.add_test_cases_from_csv_file(\n",
    "        file_path=f'{V_SYNTHETIC}/{EXPERIMENT_NAME}_complete_ragas_synthetic_data.csv', # function\n",
    "        input_col_name=\"input\",\n",
    "        actual_output_col_name=\"actual_output\",\n",
    "        expected_output_col_name=\"expected_output\",\n",
    "        context_col_name=\"context\",\n",
    "        context_col_delimiter= \";\",\n",
    "        retrieval_context_col_name=\"retrieval_context\",\n",
    "        retrieval_context_col_delimiter= \";\"\n",
    "    )\n",
    "\n",
    "    # Instantiate retrieval metrics\n",
    "    contextual_precision = ContextualPrecisionMetric(\n",
    "        threshold=0.5, # default is 0.5\n",
    "        model=\"gpt-4o\",\n",
    "        include_reason=True\n",
    "    )\n",
    "\n",
    "    contextual_recall = ContextualRecallMetric(\n",
    "        threshold=0.5, # default is 0.5\n",
    "        model=\"gpt-4o\",\n",
    "        include_reason=True\n",
    "    )\n",
    "\n",
    "    contextual_relevancy = ContextualRelevancyMetric(\n",
    "        threshold=0.5, # default is 0.5\n",
    "        model=\"gpt-4o\",\n",
    "        include_reason=True\n",
    "    )\n",
    "\n",
    "    # Instantiate generation metrics\n",
    "    answer_relevancy = AnswerRelevancyMetric(\n",
    "        threshold=0.5, # default is 0.5\n",
    "        model=\"gpt-4o\",\n",
    "        include_reason=True\n",
    "    )\n",
    "\n",
    "    faithfulness = FaithfulnessMetric(\n",
    "        threshold=0.5, # default is 0.5\n",
    "        model=\"gpt-4o\",\n",
    "        include_reason=True\n",
    "    )\n",
    "\n",
    "    hallucination = HallucinationMetric(\n",
    "        threshold=0.5, # default is 0.5\n",
    "        model=\"gpt-4o\",\n",
    "        include_reason=True\n",
    "    )\n",
    "\n",
    "    eval_results = evaluate(\n",
    "        test_cases=dataset,\n",
    "        metrics=[\n",
    "            contextual_precision,\n",
    "            contextual_recall,\n",
    "            contextual_relevancy,\n",
    "            answer_relevancy,\n",
    "            faithfulness,\n",
    "            hallucination\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    with open(f\"{V_RESULTS}/{EXPERIMENT_NAME}_generation_eval_results\", \"wb\") as f:\n",
    "        pickle.dump(eval_results, f)\n",
    "\n",
    "    with open(f\"{V_RESULTS}/{EXPERIMENT_NAME}_generation_eval_results\", \"rb\") as f:\n",
    "        eval_results = pickle.load(f)\n",
    "\n",
    "    metric_type = {\n",
    "    \"metric_name\": [\"Contextual Precision\", \"Contextual Recall\", \"Contextual Relevancy\", \"Answer Relevancy\", \"Faithfulness\", \"Hallucination\"],\n",
    "    \"metric_type\": [\"retrieval\", \"retrieval\", \"retrieval\", \"generation\", \"generation\", \"generation\"]}\n",
    "\n",
    "\n",
    "    evaluation = (\n",
    "        pd.DataFrame.from_records(asdict(result) for result in eval_results)\n",
    "        .explode(\"metrics_metadata\")\n",
    "        .reset_index(drop=True)\n",
    "        .assign(\n",
    "            metric_name=lambda df: df.metrics_metadata.apply(getattr, args=[\"metric\"]),\n",
    "            score=lambda df: df.metrics_metadata.apply(getattr, args=[\"score\"]),\n",
    "            reason=lambda df: df.metrics_metadata.apply(getattr, args=[\"reason\"]),\n",
    "        )\n",
    "        .merge(pd.DataFrame(metric_type), on=\"metric_name\")\n",
    "        .drop(columns=[\"success\", \"metrics_metadata\"])\n",
    "    )\n",
    "\n",
    "    evaluation.to_csv(f\"{V_RESULTS}/{EXPERIMENT_NAME}_generation_eval_results.csv\", index=False)\n",
    "    evaluation.head()\n",
    "        \n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Load CSV of experiments. See Google Drive folder 'experiment_parameters' for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_parameters = pd.read_csv('data/experiment_parameters/prompt_chunk_experiment_dataset.csv')\n",
    "\n",
    "# # # Filter by experiment name if you wish to only run certain experimental parameters\n",
    "# experiment_parameters = experiment_parameters[(experiment_parameters.experiment_name == 'original_prompt') |\n",
    "#                                               (experiment_parameters.experiment_name == 'unhelpful_prompt')]\n",
    "\n",
    "experiment_parameters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Loop through experiments and pass parameters to each function, returning the concantenated evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in experiment_parameters.iterrows():\n",
    "\n",
    "    EXPERIMENT_NAME = row[\"experiment_name\"]\n",
    "    RETRIEVAL_SYSTEM_PROMPT = row[\"retrieval_system_prompt\"]\n",
    "    RETRIEVAL_QUESTION_PROMPT = row[\"retrieval_question_prompt\"]\n",
    "    CHUNK_SIZE = row['chunk_size']\n",
    "\n",
    "    write_rag_results(V_SYNTHETIC, EXPERIMENT_NAME, RETRIEVAL_SYSTEM_PROMPT, RETRIEVAL_QUESTION_PROMPT)\n",
    "\n",
    "    do_evaluation(V_SYNTHETIC, V_RESULTS, EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Load and visualise results\n",
    "Note: there are some complexities that could require additional analysis, such as the uncertainty associated with each individual LLM judge score and the wide range of scores (0 to 1 for some metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "experiments = []\n",
    "\n",
    "# baseline = pd.read_csv(f\"{V_RESULTS}/baseline.csv\")\n",
    "# baseline['experiment_name'] = 'baseline'\n",
    "# experiments.append(baseline)\n",
    "\n",
    "# Comment out if you only want to view baseline statistics\n",
    "# Populate with experiment names\n",
    "experiment_names = ['original_prompt', 'unhelpful_prompt']\n",
    "for experiment_name in experiment_names:\n",
    "    experiment = pd.read_csv(f\"{V_RESULTS}/{experiment_name}_generation_eval_results.csv\")\n",
    "    experiment['experiment_name'] = experiment_name\n",
    "    experiments.append(experiment)\n",
    "\n",
    "experiments_df = pd.concat(experiments)\n",
    "\n",
    "def empirical_ci(df):\n",
    "\n",
    "    df_grouped = (df\n",
    "                  .groupby([\"experiment_name\", \"metric_name\"])['score']\n",
    "                  .agg([\"mean\", 'sem', 'min', 'max', 'count'])\n",
    "                  .reset_index()\n",
    "                  )\n",
    "        \n",
    "    ci = stats.t.interval(confidence=0.95, \n",
    "                          df=df_grouped['count']-1,\n",
    "                          loc=df_grouped['mean'],\n",
    "                          scale=df_grouped['sem'])\n",
    "\n",
    "    df_grouped['ci_low'] = ci[0]\n",
    "    df_grouped['ci_high'] = ci[1] \n",
    "\n",
    "    return df_grouped\n",
    "\n",
    "# Note that the confidence intervals in sns.barplot is calculated by bootstrapping. \n",
    "# See empirical_ci() above for empirical confidence interval calculation. \n",
    "sns.barplot(experiments_df, x=\"score\", y=\"metric_name\", hue='experiment_name', errorbar=('ci', 95))\n",
    "\n",
    "experiment_metrics = empirical_ci(experiments_df)\n",
    "experiment_metrics.to_csv(f\"{V_RESULTS}/{experiment_name}_eval_results_full.csv\")\n",
    "experiment_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
